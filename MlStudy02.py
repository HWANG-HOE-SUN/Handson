# Handson chap02

# -*- coding: utf-8 -*-
"""Housing문제.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ItQpat6BReYfjuQ0GatFpy1Q4SW8JSdb

# 주택가격예측 프로젝트(Hands on ML - 2장)
"""

# 패키지 설치

import pandas as pd

import matplotlib,numpy,scipy,sklearn

# Commented out IPython magic to ensure Python compatibility.
# 파이썬 2와 파이썬 3 지원
from __future__ import division, print_function, unicode_literals

# 공통
import numpy as np
import os

# 일관된 출력을 위해 유사난수 초기화
np.random.seed(42)

# 맷플롯립 설정
# %matplotlib inline
import matplotlib
import matplotlib.pyplot as plt
plt.rcParams['axes.labelsize'] = 14
plt.rcParams['xtick.labelsize'] = 12
plt.rcParams['ytick.labelsize'] = 12

# 한글출력
matplotlib.rc('font', family='NanumBarunGothic')
plt.rcParams['axes.unicode_minus'] = False

# 그림을 저장할 폴드
PROJECT_ROOT_DIR = "."
CHAPTER_ID = "end_to_end_project"
IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, "images", CHAPTER_ID)

def save_fig(fig_id, tight_layout=True, fig_extension="png", resolution=300):
    path = os.path.join(IMAGES_PATH, fig_id + "." + fig_extension)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format=fig_extension, dpi=resolution)

# 매번 Data달라질 수 있기에, 자동으로 Data불러와서 디렉토리 만들고 압축푸는 함수임

import os
import tarfile
from six.moves import urllib

DOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml/master/"
HOUSING_PATH = os.path.join("datasets", "housing")
HOUSING_URL = DOWNLOAD_ROOT + "datasets/housing/housing.tgz"

def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
    if not os.path.isdir(housing_path):
        os.makedirs(housing_path)
    tgz_path = os.path.join(housing_path, "housing.tgz")
    urllib.request.urlretrieve(housing_url, tgz_path)
    housing_tgz = tarfile.open(tgz_path)
    housing_tgz.extractall(path=housing_path)
    housing_tgz.close()

fetch_housing_data()

def load_housing_data(housing_path=HOUSING_PATH):
    csv_path = os.path.join(housing_path, "housing.csv")
    return pd.read_csv(csv_path)

"""## 1. 데이터 기본탐색"""

housing = load_housing_data()
housing.head()

housing.info() # Not-Null인 데이터수 몇개? 타입은 몇개? 전체갯수는? 확인가능 ->20640행임

housing["ocean_proximity"].value_counts() #housing의 유일한 not숫자형속성, ocean~의 카테고리별 갯수 확인

"""### 함수 정리
#### value_counts() = 범주(카테고리) 정보 요약해주는 놈이라 보면 됨.
#### describe() = 숫자형 특성의 요약 정보를 보여줌.(갯수, 평균, 사분위수, 최대최소값 등)
#### info() = DataFrame의 속성별 특징, na값을 파악해서 보여줌.
#### set_index : 기존의 행 인덱스를 제거하고 데이터 열 중 하나를 인덱스로 설정 Ex) data.set_index("c1") # c1=열이름
#### reset_index : 기존의 행 인덱스를 제거하고 인덱스를 데이터 열로 추가
#### ceil : 올림함수
#### where(numpy) : numpy의 조건문
#### train_test_split(data, size, random_state) : data에서 train,test 분리시킴 
#### data=전체data , size=test의비율, random~=난수시드. 출력값 2개
"""

housing.describe()

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib as plt
housing.hist(bins=50, figsize=(20,15)) #bins=각 막대가 가질수있는 너비설정. (특정수치)/bins 만큼으로 결정된다 보면됨
#즉, bins크기와 막대너비(두꼐)는 반비례  #figsize? (x축,y축)비율이라치믄댐. 20;15니까 지금 그래프 가로길이가 더 긴거임.

"""## 2. 테스트 데이터 분리(함수를 통한)"""

def split_train_test(data, test_ratio):
    shuffled_indices = np.random.permutation(len(data))
    test_set_size = int(len(data) * test_ratio)
    test_indices = shuffled_indices[:test_set_size]
    train_indices = shuffled_indices[test_set_size:]
    return data.iloc[train_indices], data.iloc[test_indices]

train_set, test_set = split_train_test(housing, 0.2)
print(len(train_set), "train +", len(test_set), "test")

"""### 테스트데이터 일정하게 유지 ※ hands on ml Page86 -> 새data 업뎃되도 기존꺼 기억하는 방법도 있음)"""

# 난수 인덱스 np.random.seed(42)등으로 test_data가 프로그램 생성시마다 달라지는 문제 방지 가능
# But, 새로운 data가 계속 업뎃되면 문제됨. 예로)샘플마다 고유한 식별자가 있다면, 이걸로 불변의 test data 유지가능
# 그 예가 바로, 식별자의 해시값을 통한 분리이다. 아래에서 해당 코드를 구현한다.

from zlib import crc32

def test_set_check(identifier, test_ratio):
    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32

def split_train_test_by_id(data, test_ratio, id_column):
    ids = data[id_column]
    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))
    return data.loc[~in_test_set], data.loc[in_test_set]

housing_with_id = housing.reset_index() #식별자 열이 없으므로 행인덱스(번호)를 사용한다. 
#이 문장으로 index열이 추가된 DF(데이터프레임) 반환됨.

housing_with_id["id"] = housing["longitude"]*1000 + housing["latitude"]
train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, "id")

"""### 사이킷런 함수를 통한 train,test 분리( train_test_split함수 ) - 위보단 간단한 방법"""

from sklearn.model_selection import train_test_split
train_set, test_set = train_test_split(housing, test_size=0.2, random_state = 42)

"""### 계층샘플링 방식(모집단 대표성을 위해, 계층별로 뽑겠다)
#### 위 문제의 경우, 블록(미국지리구역)단위 중간소득 -> 중간주택가격(목표치) 잘 예측하는 변수니, 중간소득 계층별로 샘플링함이 유의미함
"""

housing["income_cat"]= np.ceil(housing["median_income"]/1.5) #ceil문 이하의 뜻 = 1.5로 나눈 소득값을 '올림'하겠다.(이산적 범주화 위해)
housing["income_cat"].where(housing["income_cat"]<5, 5.0, inplace=True) #5보다 큰건 5.0으로 쳐라. inplace=T? 원본배열 변경
# where조건에서 <5로 했다는건... 5까지만 치겠다는거고, 5보다 큰건 5.0으로 자동 전환된다는 의미임.

"""#### 사이킷런의 계층샘플링 함수. StratifiedShuffleSplit 계층화된 무작위(섞인) 스플릿"""

from sklearn.model_selection import StratifiedShuffleSplit

split = StratifiedShuffleSplit(n_splits=1 , test_size = 0.2, random_state = 42)
for train_index, test_index in split.split(housing,housing["income_cat"]):
    strat_train_set = housing.loc[train_index]
    strat_test_set = housing.loc[test_index]

housing["income_cat"].value_counts()/len(housing) #소득카테고리별, 전체 차지하는 비율(인구대비)

"""#### ★아래의 첫번째 방식을 사용하는것 추천!"""

strat_test_set["income_cat"].value_counts()/len(strat_test_set) # 계층 추출로 만든 Test데이터 소득비율. 모집단과 거의 일치

"""#### income_cat제거 (test제대로 만들어졌나 확인하는 목적 달성했으니 되돌리겠음)"""

for set_ in(strat_train_set, strat_test_set):
    set_.drop("income_cat",axis=1, inplace=True) #drop axis의 default=0(행) 1은 열을 버리겠다는 의미.

"""## 3. 데이터 탐색 및 시각화

### 3-1. 기본 특성(속성) 별 시각화
"""

housing = strat_train_set.copy() # 훈련Data를 손상시키지 않고 자유로운 탐색을 위해, 따로 복사본을 만들어 쓰겠다.

"""#### 3-1.1 위치별 시각화"""

# <데이터 수가 많은 곳일수록 진하다.(즉, 인구밀집이 심할수록) >
housing.plot(kind="scatter", x="longitude", y="latitude",alpha=0.1) #경도,위도에 따른 지리정보(scatter.점모양)
# alpha는 투명도 0(완전투명)~1(불투명) 흐리게하면, 그만큼 점 몰린곳이 안흐리게 보여서 구분이 잘감.

import matplotlib.pyplot as plt

housing.plot(kind="scatter", x="longitude", y="latitude", alpha=0.4, s=housing["population"]/100, label="population"
            ,figsize=(10,7),c="median_house_value",cmap=plt.get_cmap("jet"),colorbar=True,sharex=False) # s=size(점크기) c=color
# cmap = plt.get_cmap("jet") 나 cmap="jet"나 같은 결과임.
# sharex 모든 서브플롯이 같은 x축 눈금을 사용하도록 xlim을 조정하면 모든 서브플롯에 조정 
plt.legend(0) # 밀집지역이 집값도 비싸다. + 해안근처주택은 높은곳도, 안높은곳도 있어서 단순히 규칙적용하기 애매함.

corr_matrix = housing.corr()
corr_matrix # 모든 변수간의 상관관계행렬 출력함
corr_matrix["median_house_value"].sort_values(ascending=False) #중간주택가격에 대한 다른 변수들 관계만, 정렬해서 출력(내림차순)

"""#### 산점도 행렬"""

from pandas.plotting import scatter_matrix
attributes = ["median_house_value","median_income","total_rooms","housing_median_age"]
scatter_matrix(housing[attributes],figsize=(12,8))

# <중간주택가격과 가장 상관성높은 중간소득과의 관계 보겠다.> - 위 그램 확대

"""#### 3-1-2. 주택소득별 시각화 (산점도행렬 분석을 통해 얻은)"""

housing.plot(kind="scatter", x="median_income", y="median_house_value", alpha=0.1)

"""### 3-2. 기본속성 조합을 통한 시각화(새로운 변수 속성 생성)"""

housing["rooms_per_household"] = housing["total_rooms"]/housing["households"] #가구수 대비 전체 방수
housing["bedrooms_per_room"] = housing["total_bedrooms"]/housing["total_rooms"] # 가구수 대비 전체 가구(침대)수
housing["population_per_household"]=housing["population"]/housing["households"] # 가구수 대비 블록 전체 인구(사람수)

corr_matrix=housing.corr()
corr_matrix["median_house_value"].sort_values(ascending=False)

"""## 4. 머신러닝 알고리즘을 위한 데이터 준비

### 위 과정에서 유용한 속성을 찾고, 탐색하는 걸 마쳤으니, 이제 훈련용 data에서 x, y(label)분리해서, 훈련시키겠다.
"""

housing = strat_train_set.drop("median_house_value",axis=1) # label값 제거해서 다시금 housing에 저장
housing_labels = strat_train_set["median_house_value"].copy() #label값 따로 보관

"""### 4-1. 데이터 전처리(정제)

#### 누락값(NA) 처리
"""

# 누락값 처리방법 3가지
housing.dropna(subset=["total_bedrooms"]) #아예 제거하는 방법 (옵션1-NA값만 제거)
housing.drop("total_bedrooms",axis=1) #누락값 있는 전체 속성을 삭제 (옵션2-NA속성열 전체삭제)
# ★계산된 중간값도 반드시 따로 저장할 것!!
median = housing["total_bedrooms"].median() # 중간값을 계산해서 (옵션3- 대체)
housing["total_bedrooms"].fillna(median, inplace=True) #누락값을 대체함.

"""#### 누락값 대체 보충"""

from sklearn.impute import SimpleImputer# 누락된 값 처리(대체)를 도와주는 사이킷런의 패키지
imputer = SimpleImputer(strategy="median") # 여러 전략중, 대체전략을 한번 써보겠음.
# default전략=mean, 그 외엔 most_frequent(최빈값)과 constant(직접 지정한 상수로 채움)가 있음.

housing_num = housing.drop("ocean_proximity",axis=1) # 유일하게 수치가아닌 헤안가여부 변수만 드랍함.

imputer.fit(housing_num) #대체전략을 훈련data에 적용
# <지금은 total_bedrooms만 누락값이 있으나 추후 어디에 Na생길지 모르니 모든 NUM특성에 imputer적용하는것이 좋다.>
# <imputer는 각 특성의 중간값을 계산해서 그 결과를 객체의 statistics_속성에 저장한다.>

print(imputer.statistics_)
print(housing_num.median()) # .values()를 붙이면 위의 값과 동일하게 나옴.

# 실제 대체 괴정
X = imputer.transform(housing_num) #변형된 특성들이 들어있는 numpy배열이 X에 저장되어있음.
# imputer를 통해 transform 하는 과정에서 DF였던 housing_num이 numpy배열이 되었다. (왜일까?)
housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=list(housing.index.values))
housing_tr.info() # NA값이 다 제거 됐다.

"""### 함수 정리 (7/29)
 
 #### corr() - data.corr()로 사용. 전체 상관계수 행렬 출력
 #### scatter_matrix (pandas.plotting 에서 옴) - scatter_matrix[DF[속성]]으로 사용. 속성들간 관계를 점으로 파악해줌.
 #### ★★ https://databuzz-team.github.io/2018/11/11/make_pipeline/ (변환기설명)
 #### Imputer는 변환기의 예시.(결측값대체용. 인자는 strategy=mean(default)
 #### factorize = 범주형 열을 정수형 값으로 바꿔준다.
 #### imputer.fit(train데이터) , X = imputer.transform(train데이터)
 #### https://wikidocs.net/89 파이썬 메서드 관련 부가설명
 #### https://docs.python.org/ko/3/tutorial/inputoutput.html format()메서드(7.1.2)  외 다른 여러가지 설명
"""

# 그 외 참고 사이트
# https://rfriend.tistory.com/345 reshape() 함수 설명
# https://m.blog.naver.com/PostView.nhn?blogId=wideeyed&logNo=221343373342&proxyReferer=https:%2F%2Fwww.google.com%2F 원핫인코딩 설명
# https://www.inflearn.com/questions/4642 생성자(for 변환기 생성을 위함) 설명글 (질문/답변)

"""#### 텍스트/범주형 데이터 처리

우선 텍스트를 숫자(정수형)으로 factorize를 이용해 바꾼후, OneHotEncoder해줄걸 다운받아서 그대로 사용함
"""

housing_cat = housing['ocean_proximity']
housing_cat_encoded, housing_categories = housing_cat.factorize()
housing_cat_encoded[:10]

housing_categories

from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder(categories='auto') # 여기서 sparse=True(default)를 False로하면 toarray할 필요없이 바로 아래줄 결과 내놓음
housing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(-1,1))
housing_cat_1hot

housing_cat_1hot.toarray() # 희소행렬을 배열형태로(숫자) 바꿨다.

# 클래스 생성 연습임
class Student:
    def __init__(self):
        print("학생 객체 받아라!")
    
    def setData(self, name, age, score):
        self.name=name
        self.age =age
        self.score=score
        
    def printData(self):
        print("이름: ",self.name)
        print("나이: ",self.age)
        print("점수: ",self.score)
        
    def Test(self):
        print("시험용1")

a= Student()

"""#### ※참고 - 카테고리 특성의 카테고리 수가 정말 많다면(생물종류, 국가코드, 직업등) 원-핫인코딩 입력특성 넘 많아짐
##### 이럴경우, 범주형 값을 그냥 숫자 특성을 바꾸는것 고려(ex-ocean근접성->ocean과의 거리변수로) 또는,
##### 임베딩(embedding)이란 방법을 쓸 수 있음. 이는 표현학습의 예라고 함(13,17장에서 배운다)

#### 나만의 변환기 만들기
"""

# 앞서 했던, 방별가구(침대)수, 가구수대비인구수(블록의) 등을 계산하는 파이프라인을 만든것임.(복합속성추가기)

from sklearn.base import BaseEstimator, TransformerMixin #BaseEstimator=하이퍼파라미터 튜닝에 필요한 메서드 생성해줌.
                                                          #TransformerMixin= fit_transform()메서드 생성해줌. 이 두가지 상속할거임
# 컬럼 인덱스(DataFrame을 통째로 self자리에 넣어 객체 생성후, 이 인덱스 숫자값을 통해 해당 속성의 column접근, 값을 인자로 넣어서 사용)
rooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6

class CombinedAttributesAdder(BaseEstimator, TransformerMixin): # class 클래스명(SuperClass이름):을 통해 상속가능(메서드 물려받음)
    def __init__(self, add_bedrooms_per_room = True): # no *args or **kwargs 
        self.add_bedrooms_per_room = add_bedrooms_per_room #유일한 하이퍼파라미터임.(ML모델에 도움될지 안될지 모르는거 지정하면 돼)
    def fit(self, X, y=None):
        return self  # self에 들어갈놈 지정해서 객체만 생성해줌.
    def transform(self, X, y=None):
        rooms_per_household = X[:, rooms_ix] / X[:, household_ix]
        population_per_household = X[:, population_ix] / X[:, household_ix]
        if self.add_bedrooms_per_room:
            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]
            return np.c_[X, rooms_per_household, population_per_household, #X에 복합속성 3개를 열로서 추가한다. (c_ 기능 이용)
                         bedrooms_per_room]
        else:
            return np.c_[X, rooms_per_household, population_per_household] #방당침대수 생성안되면 나머지 2개만 열로 추가해서 반환

attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)
housing_extra_attribs = attr_adder.transform(housing.values)



"""#### 특성 스케일링(표준화/정규화)"""

# 표준화(standardization)-통계에서 배운 그것. 평균과의 거리가 얼마나 떨어져 있나를 보게 해주는 목적. 평균이0 표편으로 나눔
# 서로 다른 단위의 분포들도 비교가능하게 해줌(키vs몸무게 등) 값-평균/표준편차 (값의 상한,하한이 정해져있지 않다는 특징!!)
#구현하는 변환기 - StandardSccaler

#정규화(normalization)- 데이터 스케일이 0~1범위에 모두 들도록 바꾸는 것. 계산방식이 위와 다름.
# 구현하는 변환기 - MinMaxScaler(feature_range에서 0~1이아닌 다른 범위로 조절 가능)

"""#### 변환 파이프라인(변환단계 여럿인데, 이를 잘 수행하도록... 여러 변환기 이어줌)

##### 수치특성을 전처리(스케일링) 하기 위한 변환 파이프라인
"""

# 내 사이킷런 버전이 0.20?이 아니기 떄문에 지금은 이게 안돌아감.
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

num_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy="median")),
        ('attribs_adder', CombinedAttributesAdder()),
        ('std_scaler', StandardScaler()),
    ])

housing_num_tr = num_pipeline.fit_transform(housing_num)

#열들을 수치든 범주형이든 알아서 다 적용해서 전처리 해주는 기능임
from sklearn.compose import ColumnTransformer

num_attribs = list(housing_num)
cat_attribs = ["ocean_proximity"]

full_pipeline = ColumnTransformer([
        ("num", num_pipeline, num_attribs),
        ("cat", OneHotEncoder(categories='auto'), cat_attribs),
    ])

housing_prepared = full_pipeline.fit_transform(housing)
housing_prepared

"""## 5 모델 선택과 훈련

### 훈련세트에서 훈련하고 평가해보기
"""

from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()  
lin_reg.fit(housing_prepared, housing_labels) #앞선 파이프라인으로 변환된 Data와 따로 빼둔 라벨을 넣음으로 모델훈련 끝.

some_data = housing.iloc[:5] # 5개의 Data의 중간주택가격 예측해 보겠음.
some_labels = housing_labels.iloc[:5]
some_data_prepared = full_pipeline.transform(some_data) #train으로 fit해둔 파이프라인에, 5개 소수값 넣어서 무슨 예측값 나오는지 봄
print("예측", lin_reg.predict(some_data_prepared))
print("레이블:", list(some_labels))

"""### 정확도 측정"""

from sklearn.metrics import mean_squared_error
housing_predictions = lin_reg.predict(housing_prepared)
lin_mse = mean_squared_error(housing_labels, housing_predictions)
lin_rmse = np.sqrt(lin_mse)
lin_rmse #평균 제곱근 오차가 68628임. 매우 큼. 다른 모델 써보자, ==>>예측력이 떨어졌단건?(train에 과소적합된 상태)

#과소적합시? 1)더 좋은 모델써보기(디시젼트리등)  2)훈련 알고리즘에 더 좋은 특성 주입  3)모델의 규제 감소(여기선 없었음)

"""### 다른 모델 써보기(개선할만한 모델 탐색)"""

from sklearn.tree import DecisionTreeRegressor

tree_reg = DecisionTreeRegressor()
tree_reg.fit(housing_prepared, housing_labels) # 디시젼 트리로 훈련 완료

"""#### 다시금, 테스트"""

housing_predictions = tree_reg.predict(housing_prepared)
tree_mse = mean_squared_error(housing_labels, housing_predictions)
tree_rmse = np.sqrt(tree_mse)
tree_rmse #0.0이나옴 이번엔 너~무 과대적합.

"""### ★교차검증을 사용한 평가"""

# 훈련세트 -> 더 작은 훈련세트,검증세트... 더 작은 훈련세트로 훈련시킨후, 검증세트로 모델 평가해 보겠다.
# 사이킷런의 k-겹 교차검증 기능도 있다.
#복원추출, 훈련세트를 10개의 폴드(서브셋임)로 무작위 분할. 10번 훈련하고 평가할건데 매번 다른 폴드를 평가용, 그 외 9개폴드 훈련용임.

from sklearn.model_selection import cross_val_score
scores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring="neg_mean_squared_error", cv=10)
tree_rmse_scores = np.sqrt(-scores)
tree_rmse_scores

#선형모형과 비교를 위해 이놈도 교차검증 해보겠음.
lin_scores = cross_val_score(lin_reg,housing_prepared, housing_labels, scoring="neg_mean_squared_error", cv=10)
lin_rmse_scores = np.sqrt(-lin_scores)

def display_scores(scores):
    print("점수:",scores)
    print("평균:",scores.mean())
    print("표준편차:", scores.std())

display_scores(tree_rmse_scores) #점수 70977+-3085 선형모델보다 더 크게 틀림. 훈련세트점수(0)에비해
#검증 세트점수(70977)가 압도적으로 높으니, 무지하게 과대적합 되어있는거임.
#과대적합 해결하려면? 하이퍼파라미터튜닝 하거나(즉, 모델을 더 단순하게 만드는 규제를 가하거나)
# 더 많은 데이터를 모아야함. (또는 그냥 다른 모델을 더 시도... 참고로 랜덤포레스트는 훨씬 나은 결과 뜸. 그것도 과대지만)

display_scores(lin_rmse_scores) #점수 69052+-2731 이놈이 조금 더 낫다.

"""## 6. 선택된 모델 세부 튜닝"""

# 앞선 과정으로 모델을 추렸다고 가정하겠음.(몇 가지를)

"""### 6-1. 그리드 탐색(적은 수의 하이퍼파라미터 조합 탐구할때 good)"""

#랜덤포레스트 하이퍼파라미터 추정예시(어떤 값의 조합이 제일 좋을까? 교차검증을 통해 평가해줌.-사이킷런의 GridSearchCV를통해)



from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestRegressor
# GridSearch가 찾을 parameter들을 이렇게 정의해 줘야함.
# dict형으로 이렇게 할떈 'string파라미터명':[리스트,해당값들] 형태이다.
param_grid = [ 
    {'n_estimators':[3,10,30], 'max_features':[2,4,6,8]},
    {'bootstrap':[False],'n_estimators':[3,10],'max_features':[2,3,4]}
     ]
forest_reg = RandomForestRegressor()
grid_search = GridSearchCV(forest_reg, param_grid, cv=5,
                         scoring="neg_mean_squared_error",
                         return_train_score=True)
grid_search.fit(housing_prepared, housing_labels)

"""### 점수 평가부분은, 위의 오류때문에 작성을 생략했다.

### 6-2 랜덤탐색(많은 수의 조합을 탐구할때 사용) - 하이퍼파라미터에 임의의수 대입. 단순반복 횟수 지정해서 하면됨.

## 7 테스트 세트로 시스템 평가(마지막)
"""

# 테스트세트 -> 예측변수와 레이블 획득(x,y) -> full_pipeline변환 -> transform시행(이미 train으로 fit된 상태의 그것에)

# ★하이퍼파라미터 튜닝많이했다면, 앞서 test세트에 대해 교차검증으로 측정한것보다 성능이 낮을것임.
# Why? 실제 검증데이터에서 좋은 성능을 내도록(일반화하도록) 세밀하게 튜닝했기 때문에... 당장 test세트에서 성능 안좋더라도
# 그거 좋게하려고 애쓰진 말아라.(새로운 Data에 일반화시키는게 중요허니까)

